{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "version": "3.6.4",
      "file_extension": ".py",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "name": "python",
      "mimetype": "text/x-python"
    },
    "kaggle": {
      "accelerator": "none",
      "dataSources": [],
      "dockerImageVersionId": 30527,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": false
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install langchain openai\n",
        "!pip install -q -U faiss-cpu tiktoken"
      ],
      "metadata": {
        "id": "FCz0cvWF_wbA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import getpass\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Open AI API Key:\")"
      ],
      "metadata": {
        "id": "gnISdj8lMqKr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Introduction\n",
        "\n",
        "Large Language Models (LLMs) are powerful tools for generating human-like text, but they have limitations.\n",
        "\n",
        "Retrieval Augmented Generation (RAG) addresses these challenges, enhancing LLMs by integrating retrieval mechanisms. This approach ensures that the content LLMs produce is both contextually relevant and factually accurate. RAG acts as a bridge, connecting LLMs to vast knowledge sources. As AI becomes increasingly used for diverse tasks, the accuracy and relevance of the generated information are crucial.\n",
        "\n",
        "RAG meets this demand, making AI interactions more informative and context-aware.\n",
        "\n",
        "# What You Need for RAG Implementation\n",
        "\n",
        "Before building out a RAG system, it's essential to familiarize yourself with the tools that make this process possible.\n",
        "\n",
        "Each tool plays a specific role, ensuring that the RAG system operates efficiently and effectively.\n",
        "\n",
        "**LLM**: At the heart of the system is the LLM, the core AI model responsible for generating human-like text responses.\n",
        "\n",
        "**Vector Store**: This is where the magic happens. The Vector Store is a dedicated storage system that houses embeddings and their corresponding textual data, ensuring quick and efficient retrieval.\n",
        "\n",
        "**Vector Store Retriever**: Think of this as the search engine of the system. The Vector Store Retriever fetches relevant documents by comparing vector similarities, ensuring that the most pertinent information is always at hand.\n",
        "\n",
        "**Embedder**: Before storing or retrieving data, we need to convert textual information into a format the system can understand. The Embedder takes on this role, transforming text into vector representations.\n",
        "\n",
        "**Prompt**: Every interaction starts with a user's query or statement. The Prompt captures this initial input, setting the stage for the retrieval and generation processes.\n",
        "\n",
        "**Document Loader**: With vast amounts of data to process, the Document Loader is essential. It imports and reads documents, preparing them for chunking and embedding.\n",
        "\n",
        "**Document Chunker**: To make the data more manageable and efficient for retrieval, the Document Chunker breaks documents into smaller, more digestible pieces.\n",
        "\n",
        "**User Input**: Last but not least, the User Input tool captures the query or statement provided by the end-user, initiating the entire RAG process.\n",
        "\n",
        "\n",
        "# The RAG System and Its Subsystems\n",
        "\n",
        "The primary goal of RAG is to provide LLMs with contextually relevant and factually accurate information, ensuring that the generated content meets the highest standards of quality and relevance.\n",
        "\n",
        "To achieve this, the RAG system is divided into subsystems, each playing a crucial role in the overall process. The tools integral to the RAG system are not standalone entities; they interweave to form the subsystems that drive the RAG process.\n",
        "\n",
        "Each tool fits within one of the following subsystems:\n",
        "\n",
        "1) Index\n",
        "\n",
        "2) Retrieval\n",
        "\n",
        "3) Augment\n",
        "\n",
        "These work together as an orchestrated flow that transforms a user's query into a contextually rich and accurate response.\n"
      ],
      "metadata": {
        "id": "CiCAlO7L-VlD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Index System\n",
        "\n",
        "**Purpose:** This subsystem is responsible for preparing and organizing the data for efficient retrieval.\n",
        "\n",
        "Here are the steps of the Index system\n",
        "\n",
        "**1) Load Documents (Document Loader)**: Imports and reads the vast amounts of data that the system will use.\n",
        "\n",
        "**2) Chunk Documents (Document Chunker):** Breaks down the loaded documents into smaller, more manageable pieces to facilitate efficient retrieval.\n",
        "\n",
        "**3) Embed Documents (Embedder):** Converts these textual chunks into vector representations, making them searchable within the system.\n",
        "\n",
        "**4) Store Embeddings (Vector Store):** Safely stores the generated embeddings alongside their textual counterparts for future retrieval."
      ],
      "metadata": {
        "id": "S9o1pzeRkXUT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load documents"
      ],
      "metadata": {
        "id": "iK3gf7TDH9GS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.document_loaders import WebBaseLoader\n",
        "\n",
        "yolo_nas_loader = WebBaseLoader(\"https://deci.ai/blog/yolo-nas-object-detection-foundation-model/\").load()\n",
        "\n",
        "decicoder_loader = WebBaseLoader(\"https://deci.ai/blog/decicoder-efficient-and-accurate-code-generation-llm/#:~:text=DeciCoder's%20unmatched%20throughput%20and%20low,re%20obsessed%20with%20AI%20efficiency.\").load()\n",
        "\n",
        "yolo_newsletter_loader = WebBaseLoader(\"https://deeplearningdaily.substack.com/p/unleashing-the-power-of-yolo-nas\").load()"
      ],
      "metadata": {
        "id": "nmKgqy5X_vhA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Chunk documents"
      ],
      "metadata": {
        "id": "H1shOpId_vqb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size = 500,\n",
        "    chunk_overlap = 50,\n",
        "    length_function = len\n",
        ")\n",
        "\n",
        "yolo_nas_chunks = text_splitter.transform_documents(yolo_nas_loader)\n",
        "decicoder_chunks = text_splitter.transform_documents(decicoder_loader)\n",
        "yolo_newsletter_chunks = text_splitter.transform_documents(yolo_newsletter_loader)"
      ],
      "metadata": {
        "id": "fuXm06J1IDs7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create an index\n"
      ],
      "metadata": {
        "id": "cGzRpURwJqHM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.embeddings.openai import OpenAIEmbeddings\n",
        "from langchain.embeddings import CacheBackedEmbeddings\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain.storage import LocalFileStore\n",
        "\n",
        "store = LocalFileStore(\"./cachce/\")\n",
        "\n",
        "# create an embedder\n",
        "core_embeddings_model = OpenAIEmbeddings()\n",
        "\n",
        "embedder = CacheBackedEmbeddings.from_bytes_store(\n",
        "    core_embeddings_model,\n",
        "    store,\n",
        "    namespace = core_embeddings_model.model\n",
        ")\n",
        "\n",
        "# store embeddings in vector store\n",
        "vectorstore = FAISS.from_documents(yolo_nas_chunks, embedder)\n",
        "vectorstore.add_documents(decicoder_chunks)\n",
        "vectorstore.add_documents(yolo_newsletter_chunks)\n",
        "\n",
        "# instantiate a retriever\n",
        "retriever = vectorstore.as_retriever()"
      ],
      "metadata": {
        "id": "iGtfN_hiJzVB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Retrieval System\n",
        "\n",
        "**Purpose:** As the name suggests, this subsystem fetches the most relevant information based on the user's query.\n",
        "\n",
        "Here are the steps in the Retrieval system\n",
        "\n",
        "**1) Obtain User Query (User Input):** Captures the user's question or statement.\n",
        "\n",
        "**2) Embed User Query (Embedder):** Transforms the user's query into a vector format, similar to the indexed documents.\n",
        "\n",
        "**3) Vector Search (Vector Store Retriever):** Searches the Vector Store for documents with embeddings that closely match the embedded user query.\n",
        "\n",
        "**4) Return Relevant Documents:** The system then returns the top matches, ensuring that the most pertinent information is always provided.\n"
      ],
      "metadata": {
        "id": "r4HNkh6pmQ61"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.llms.openai import OpenAIChat\n",
        "from langchain.chains import RetrievalQA\n",
        "from langchain.callbacks import StdOutCallbackHandler"
      ],
      "metadata": {
        "id": "Eq3nKNJ2mRDK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm = OpenAIChat()\n",
        "handler =  StdOutCallbackHandler()"
      ],
      "metadata": {
        "id": "D9FGhpMWORwa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# this is the entire retrieval system\n",
        "qa_with_sources_chain = RetrievalQA.from_chain_type(\n",
        "    llm=llm,\n",
        "    retriever=retriever,\n",
        "    callbacks=[handler],\n",
        "    return_source_documents=True\n",
        ")"
      ],
      "metadata": {
        "id": "uBhaA_nkpU_2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Augment System\n",
        "\n",
        "**Purpose:** This subsystem enhances the LLM's input prompt with the retrieved context, ensuring that the model has all the necessary information to generate a comprehensive response.\n",
        "\n",
        "**1) Create Initial Prompt (Prompt):** Starts with the original user query or statement.\n",
        "\n",
        "**2) Augment Prompt with Retrieved Context:** Merges the initial prompt with the context retrieved from the Vector Store, creating an enriched input for the LLM.\n",
        "\n",
        "**3) Send Augmented Prompt to LLM:** The enhanced prompt is then fed to the LLM.\n",
        "\n",
        "**4) Receive LLM's Response:** After processing the augmented prompt, the LLM generates and returns its response.\n"
      ],
      "metadata": {
        "id": "ZAaIa-kpsHHI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "response = qa_with_sources_chain({\"query\":\"What does Neural Architecture Search have to do with how Deci creates its models?\"})"
      ],
      "metadata": {
        "id": "Q-NO6i0YS6nY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(response['result'])"
      ],
      "metadata": {
        "id": "mV8rFTyST_nD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response = qa_with_sources_chain({\"query\":\"What is DeciCoder\"})"
      ],
      "metadata": {
        "id": "0-3BqcQJTBu2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(response['result'])"
      ],
      "metadata": {
        "id": "PbsKa0nKu-XP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response = qa_with_sources_chain({\"query\":\"Write a blog about Deci and how it used NAS to generate YOLO-NAS and DeciCoder\"})"
      ],
      "metadata": {
        "id": "6RwqP62Uu9Tc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(response['result'])"
      ],
      "metadata": {
        "id": "qivMMWHMvFqe"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}